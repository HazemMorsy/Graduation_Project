{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Librosa.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmbgJuuHxEKH","executionInfo":{"status":"ok","timestamp":1620834463026,"user_tz":-120,"elapsed":1664,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"a91e1fb2-2063-471d-d3ed-c78c509e0374"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIo7Cr_3TTWK","executionInfo":{"status":"ok","timestamp":1620834467372,"user_tz":-120,"elapsed":3743,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"b9303e3e-2096-4bc8-ccec-f7989a0309ff"},"source":["pip install librosa"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n","Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (20.9)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (56.1.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa) (1.14.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa) (2.4.7)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lHSZNSoESs75"},"source":["import librosa\n","import librosa.display\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from matplotlib.pyplot import specgram\n","import keras\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import LSTM\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Input, Flatten, Dropout, Activation\n","from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.metrics import confusion_matrix\n","from keras import regularizers\n","import os\n","from os import path\n","\n","from tensorflow import keras\n","from keras.layers import Dense , Dropout\n","from keras.wrappers.scikit_learn import KerasRegressor\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from scipy.stats import pearsonr\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.utils import shuffle\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SuEaW35TaOfg","executionInfo":{"status":"ok","timestamp":1620834471608,"user_tz":-120,"elapsed":1369,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"72fbf46c-fb8a-43a4-8c4b-79da946900d4"},"source":["cd '/content/drive/MyDrive/College/Grad Project/MIT/Audios_Answers'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1vMfKGnS1lhSz-f1ELRTPdbnHyeg69WAI/Grad Project/MIT/Audios_Answers\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JS2CkZ5pUFS1"},"source":["pretrained_model_path = '/content/drive/MyDrive/College/Grad Project/MIT/Pretrained Models/Emotion_Voice_Detection_Model.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkjuD9mjUAto","executionInfo":{"status":"ok","timestamp":1620834474084,"user_tz":-120,"elapsed":827,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"d29f671f-7d78-464d-9493-c269963d6625"},"source":["pretrained_model = keras.models.load_model(pretrained_model_path)\n","print(len(pretrained_model.layers))\n","pretrained_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["18\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_7 (Conv1D)            (None, 216, 128)          768       \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 216, 128)          0         \n","_________________________________________________________________\n","conv1d_8 (Conv1D)            (None, 216, 128)          82048     \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 216, 128)          0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 216, 128)          0         \n","_________________________________________________________________\n","max_pooling1d_2 (MaxPooling1 (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_9 (Conv1D)            (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_10 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_10 (Conv1D)           (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_11 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_11 (Conv1D)           (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_12 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_12 (Conv1D)           (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_13 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 3456)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                34570     \n","_________________________________________________________________\n","activation_14 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 445,578\n","Trainable params: 445,578\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4nzpIi57ULcA"},"source":["def my_model():\n","  # new_pretrained_model = pretrained_model #keras.Sequential(pretrained_model.layers[:-1])   #pretrained_model\n","  new_pretrained_model = keras.Sequential(pretrained_model.layers[:-2])   #pretrained_model\n","  for layer in new_pretrained_model:\n","    layer.trainable = False\n","  \n","  new_pretrained_model.add(Dense(512 , activation='relu' , name='added_dense_1'))\n","  new_pretrained_model.add(Dropout(0.25 , name = 'added_dropout'))\n","  new_pretrained_model.add(Dense(1 , kernel_regularizer=keras.regularizers.l2(l2=0.01) , name='added_dense_2'))\n","  new_pretrained_model.compile(loss='mean_squared_error' , optimizer = 'adam')#optimizer=keras.optimizers.Adam(learning_rate=0.001))\n","  return new_pretrained_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSoop8GtULcC"},"source":["estimator = KerasRegressor(build_fn=my_model , epochs=300 , batch_size = 32 , verbose=1 )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzZTLjg9U_0S"},"source":["# PreProcessing Features"]},{"cell_type":"code","metadata":{"id":"5j2T7kHY409U"},"source":["def get_paths_list(identifier = 'P'):\n","\n","  interview_paths = []\n","  for interview in range(90):\n","    for question in range(5):\n","      path_str = identifier + str(interview+1) + '/' + identifier + str(interview+1) + 'Q' + str(question+1) + '.wav'\n","      \n","      if path.exists(path_str):\n","        interview_paths.append(path_str)\n","\n","  return interview_paths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgv0ivBd_Y5w"},"source":["def get_all_paths():\n","\n","  return get_paths_list('P') + get_paths_list('PP')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGhCrhUcWZ5y"},"source":["# getting features with librosa"]},{"cell_type":"code","metadata":{"id":"Sdz3XNQQWZlt"},"source":["# df = pd.DataFrame(columns=['feature'])\n","# bookmark=0\n","# for index,y in enumerate(mylist):\n","#     if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n","#         X, sample_rate = librosa.load('RawData/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n","#         sample_rate = np.array(sample_rate)\n","#         mfccs = np.mean(librosa.feature.mfcc(y=X, \n","#                                             sr=sample_rate, \n","#                                             n_mfcc=13),\n","#                         axis=0)\n","#         feature = mfccs\n","#         #[float(i) for i in feature]\n","#         #feature1=feature[:135]\n","#         df.loc[bookmark] = [feature]\n","#         bookmark=bookmark+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSexM7fUcAnL"},"source":["df = pd.DataFrame(columns=['feature'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTSz_O9zU2dx"},"source":["def preprocess_data_P():\n","  all_data = []\n","  all_paths = get_all_paths()\n","  i = 0\n","  for one_path in all_paths[:345]: #345/// [345:]\n","    i = i + 1\n","    # read wav signal and get its sequence and sampling rate\n","    data , sampling_rate = librosa.load(one_path)#librosa.load(one_path) #wav.read(one_path)\n","    sample_rate = np.array(sampling_rate)\n","    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13), axis=0)\n","    feature = mfccs\n","    df.loc[i] = [feature]\n","    # get the average of 2 channels of the audio\n","    # print(one_path)\n","    # print(signal)\n","    # print(signal.shape)\n","    # print(sampling_rate)\n","\n","    # signal = np.mean(signal , axis=1)\n","    \n","    # pad sequences to be all in range of 55 seconds\n","\n","    #final_signal = []\n","    #final_signal.append(signal)\n","    # pad function uses list of list not a single one\n","    #signal = pad_sequences(final_signal , maxlen=1350000)\n","    #signal = signal[0]\n","\n","    # extract features\n","    #mfcc_features = mfcc(signal , sampling_rate , n_mfcc=27)\n","    #print(mfcc_features.shape)\n","    #chroma_features = chroma_stft(signal , sampling_rate)\n","    #print(chroma_features.shape)\n","    #mel_features = melspectrogram(signal , sampling_rate , n_mels = 128)\n","    #print(mel_features.shape)\n","    #contrast_features = spectral_contrast(signal , sampling_rate , n_bands =  6)\n","    #print(contrast_features.shape)\n","    #tonnetz_features = tonnetz(signal , sampling_rate)\n","    #print(tonnetz_features.shape)\n","\n","    ##########################################################################################################################\n","    # log_mels = logfbank(signal , sampling_rate , winlen=0.025 , winstep=0.01 , nfilt=40 , nfft=1200)\n","    # deltas = delta(log_mels , 2)\n","    # deltas_deltas = delta(deltas , 2)\n","    ##########################################################################################################################\n","\n","    #frame_features = np.concatenate((mfcc_features , chroma_features , mel_features , contrast_features , tonnetz_features) , axis = 0)\n","    #frame_features = np.transpose(frame_features)\n","    #frame_features = np.mean(frame_features , axis=0)\n","    #print(frame_features.shape)\n","    # append all features together\n","    ##########################################################################################################################\n","    # frame_features = np.stack((log_mels , deltas , deltas_deltas) , axis = 2)\n","    # frame_features = np.mean(frame_features , axis=0)\n","    ##########################################################################################################################\n","    # curr_noframes = frame_features.shape[0]\n","    # average_noframes = 2500\n","    # no_features = 180\n","    # padded_arr = np.zeros((average_noframes , no_features))\n","\n","    # if average_noframes >= curr_noframes:\n","    #   start_idxpad = int((average_noframes - curr_noframes) / 2)\n","    #   padded_arr[ start_idxpad : start_idxpad + curr_noframes , :] = frame_features\n","    # else:\n","    #   random_idxs = np.random.randint(curr_noframes , size=average_noframes)\n","    #   padded_arr = frame_features[random_idxs , :]\n","    \n","    # frame_features = padded_arr\n","    # print(frame_features.shape)\n","\n","    all_data.append(df)\n","    print('Done : ' + str(i))\n","\n","  \n","  all_data = np.array(all_data)\n","  np.save('/content/drive/MyDrive/College/Grad Project/MIT/training of new pretrained models/input_features_librosa_P.npy' , all_data)\n","  print(all_data.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rip54QHQZF-L","executionInfo":{"status":"ok","timestamp":1620837674633,"user_tz":-120,"elapsed":1272670,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"9f590762-6e37-44a7-b978-bf7423c215dc"},"source":["preprocess_data_P()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done : 1\n","Done : 2\n","Done : 3\n","Done : 4\n","Done : 5\n","Done : 6\n","Done : 7\n","Done : 8\n","Done : 9\n","Done : 10\n","Done : 11\n","Done : 12\n","Done : 13\n","Done : 14\n","Done : 15\n","Done : 16\n","Done : 17\n","Done : 18\n","Done : 19\n","Done : 20\n","Done : 21\n","Done : 22\n","Done : 23\n","Done : 24\n","Done : 25\n","Done : 26\n","Done : 27\n","Done : 28\n","Done : 29\n","Done : 30\n","Done : 31\n","Done : 32\n","Done : 33\n","Done : 34\n","Done : 35\n","Done : 36\n","Done : 37\n","Done : 38\n","Done : 39\n","Done : 40\n","Done : 41\n","Done : 42\n","Done : 43\n","Done : 44\n","Done : 45\n","Done : 46\n","Done : 47\n","Done : 48\n","Done : 49\n","Done : 50\n","Done : 51\n","Done : 52\n","Done : 53\n","Done : 54\n","Done : 55\n","Done : 56\n","Done : 57\n","Done : 58\n","Done : 59\n","Done : 60\n","Done : 61\n","Done : 62\n","Done : 63\n","Done : 64\n","Done : 65\n","Done : 66\n","Done : 67\n","Done : 68\n","Done : 69\n","Done : 70\n","Done : 71\n","Done : 72\n","Done : 73\n","Done : 74\n","Done : 75\n","Done : 76\n","Done : 77\n","Done : 78\n","Done : 79\n","Done : 80\n","Done : 81\n","Done : 82\n","Done : 83\n","Done : 84\n","Done : 85\n","Done : 86\n","Done : 87\n","Done : 88\n","Done : 89\n","Done : 90\n","Done : 91\n","Done : 92\n","Done : 93\n","Done : 94\n","Done : 95\n","Done : 96\n","Done : 97\n","Done : 98\n","Done : 99\n","Done : 100\n","Done : 101\n","Done : 102\n","Done : 103\n","Done : 104\n","Done : 105\n","Done : 106\n","Done : 107\n","Done : 108\n","Done : 109\n","Done : 110\n","Done : 111\n","Done : 112\n","Done : 113\n","Done : 114\n","Done : 115\n","Done : 116\n","Done : 117\n","Done : 118\n","Done : 119\n","Done : 120\n","Done : 121\n","Done : 122\n","Done : 123\n","Done : 124\n","Done : 125\n","Done : 126\n","Done : 127\n","Done : 128\n","Done : 129\n","Done : 130\n","Done : 131\n","Done : 132\n","Done : 133\n","Done : 134\n","Done : 135\n","Done : 136\n","Done : 137\n","Done : 138\n","Done : 139\n","Done : 140\n","Done : 141\n","Done : 142\n","Done : 143\n","Done : 144\n","Done : 145\n","Done : 146\n","Done : 147\n","Done : 148\n","Done : 149\n","Done : 150\n","Done : 151\n","Done : 152\n","Done : 153\n","Done : 154\n","Done : 155\n","Done : 156\n","Done : 157\n","Done : 158\n","Done : 159\n","Done : 160\n","Done : 161\n","Done : 162\n","Done : 163\n","Done : 164\n","Done : 165\n","Done : 166\n","Done : 167\n","Done : 168\n","Done : 169\n","Done : 170\n","Done : 171\n","Done : 172\n","Done : 173\n","Done : 174\n","Done : 175\n","Done : 176\n","Done : 177\n","Done : 178\n","Done : 179\n","Done : 180\n","Done : 181\n","Done : 182\n","Done : 183\n","Done : 184\n","Done : 185\n","Done : 186\n","Done : 187\n","Done : 188\n","Done : 189\n","Done : 190\n","Done : 191\n","Done : 192\n","Done : 193\n","Done : 194\n","Done : 195\n","Done : 196\n","Done : 197\n","Done : 198\n","Done : 199\n","Done : 200\n","Done : 201\n","Done : 202\n","Done : 203\n","Done : 204\n","Done : 205\n","Done : 206\n","Done : 207\n","Done : 208\n","Done : 209\n","Done : 210\n","Done : 211\n","Done : 212\n","Done : 213\n","Done : 214\n","Done : 215\n","Done : 216\n","Done : 217\n","Done : 218\n","Done : 219\n","Done : 220\n","Done : 221\n","Done : 222\n","Done : 223\n","Done : 224\n","Done : 225\n","Done : 226\n","Done : 227\n","Done : 228\n","Done : 229\n","Done : 230\n","Done : 231\n","Done : 232\n","Done : 233\n","Done : 234\n","Done : 235\n","Done : 236\n","Done : 237\n","Done : 238\n","Done : 239\n","Done : 240\n","Done : 241\n","Done : 242\n","Done : 243\n","Done : 244\n","Done : 245\n","Done : 246\n","Done : 247\n","Done : 248\n","Done : 249\n","Done : 250\n","Done : 251\n","Done : 252\n","Done : 253\n","Done : 254\n","Done : 255\n","Done : 256\n","Done : 257\n","Done : 258\n","Done : 259\n","Done : 260\n","Done : 261\n","Done : 262\n","Done : 263\n","Done : 264\n","Done : 265\n","Done : 266\n","Done : 267\n","Done : 268\n","Done : 269\n","Done : 270\n","Done : 271\n","Done : 272\n","Done : 273\n","Done : 274\n","Done : 275\n","Done : 276\n","Done : 277\n","Done : 278\n","Done : 279\n","Done : 280\n","Done : 281\n","Done : 282\n","Done : 283\n","Done : 284\n","Done : 285\n","Done : 286\n","Done : 287\n","Done : 288\n","Done : 289\n","Done : 290\n","Done : 291\n","Done : 292\n","Done : 293\n","Done : 294\n","Done : 295\n","Done : 296\n","Done : 297\n","Done : 298\n","Done : 299\n","Done : 300\n","Done : 301\n","Done : 302\n","Done : 303\n","Done : 304\n","Done : 305\n","Done : 306\n","Done : 307\n","Done : 308\n","Done : 309\n","Done : 310\n","Done : 311\n","Done : 312\n","Done : 313\n","Done : 314\n","Done : 315\n","Done : 316\n","Done : 317\n","Done : 318\n","Done : 319\n","Done : 320\n","Done : 321\n","Done : 322\n","Done : 323\n","Done : 324\n","Done : 325\n","Done : 326\n","Done : 327\n","Done : 328\n","Done : 329\n","Done : 330\n","Done : 331\n","Done : 332\n","Done : 333\n","Done : 334\n","Done : 335\n","Done : 336\n","Done : 337\n","Done : 338\n","Done : 339\n","Done : 340\n","Done : 341\n","Done : 342\n","Done : 343\n","Done : 344\n","Done : 345\n","(345, 345, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pJCLlIpUYpox"},"source":["def preprocess_data_PP():\n","  all_data = []\n","  all_paths = get_all_paths()\n","  i = 0\n","  for one_path in all_paths[345:]: #345/// [345:]\n","    i = i + 1\n","    # read wav signal and get its sequence and sampling rate\n","    data , sampling_rate = librosa.load(one_path)#librosa.load(one_path) #wav.read(one_path)\n","    sample_rate = np.array(sampling_rate)\n","    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13), axis=0)\n","    feature = mfccs\n","    df.loc[i] = [feature]\n","    # get the average of 2 channels of the audio\n","    # print(one_path)\n","    # print(signal)\n","    # print(signal.shape)\n","    # print(sampling_rate)\n","\n","    # signal = np.mean(signal , axis=1)\n","    \n","    # pad sequences to be all in range of 55 seconds\n","\n","    #final_signal = []\n","    #final_signal.append(signal)\n","    # pad function uses list of list not a single one\n","    #signal = pad_sequences(final_signal , maxlen=1350000)\n","    #signal = signal[0]\n","\n","    # extract features\n","    #mfcc_features = mfcc(signal , sampling_rate , n_mfcc=27)\n","    #print(mfcc_features.shape)\n","    #chroma_features = chroma_stft(signal , sampling_rate)\n","    #print(chroma_features.shape)\n","    #mel_features = melspectrogram(signal , sampling_rate , n_mels = 128)\n","    #print(mel_features.shape)\n","    #contrast_features = spectral_contrast(signal , sampling_rate , n_bands =  6)\n","    #print(contrast_features.shape)\n","    #tonnetz_features = tonnetz(signal , sampling_rate)\n","    #print(tonnetz_features.shape)\n","\n","    ##########################################################################################################################\n","    # log_mels = logfbank(signal , sampling_rate , winlen=0.025 , winstep=0.01 , nfilt=40 , nfft=1200)\n","    # deltas = delta(log_mels , 2)\n","    # deltas_deltas = delta(deltas , 2)\n","    ##########################################################################################################################\n","\n","    #frame_features = np.concatenate((mfcc_features , chroma_features , mel_features , contrast_features , tonnetz_features) , axis = 0)\n","    #frame_features = np.transpose(frame_features)\n","    #frame_features = np.mean(frame_features , axis=0)\n","    #print(frame_features.shape)\n","    # append all features together\n","    ##########################################################################################################################\n","    # frame_features = np.stack((log_mels , deltas , deltas_deltas) , axis = 2)\n","    # frame_features = np.mean(frame_features , axis=0)\n","    ##########################################################################################################################\n","    # curr_noframes = frame_features.shape[0]\n","    # average_noframes = 2500\n","    # no_features = 180\n","    # padded_arr = np.zeros((average_noframes , no_features))\n","\n","    # if average_noframes >= curr_noframes:\n","    #   start_idxpad = int((average_noframes - curr_noframes) / 2)\n","    #   padded_arr[ start_idxpad : start_idxpad + curr_noframes , :] = frame_features\n","    # else:\n","    #   random_idxs = np.random.randint(curr_noframes , size=average_noframes)\n","    #   padded_arr = frame_features[random_idxs , :]\n","    \n","    # frame_features = padded_arr\n","    # print(frame_features.shape)\n","\n","    all_data.append(df)\n","    print('Done : ' + str(i))\n","\n","  \n","  all_data = np.array(all_data)\n","  np.save('/content/drive/MyDrive/College/Grad Project/MIT/training of new pretrained models/input_features_librosa_PP.npy' , all_data)\n","  print(all_data.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3SbOlIEUziT"},"source":["# DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1kUBbmzUSew","executionInfo":{"status":"ok","timestamp":1620761495870,"user_tz":-120,"elapsed":662,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"65d7627b-caa2-4941-d94c-d31b3be65cb2"},"source":["cd '/content/drive/MyDrive/College/Grad Project/MIT/Audios_Answers'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1vMfKGnS1lhSz-f1ELRTPdbnHyeg69WAI/Grad Project/MIT/Audios_Answers\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8p5Il7i-USex"},"source":["def get_labels():\n","  labels_path = '/content/drive/MyDrive/College/Grad Project/MIT/turker_scores_full_interview.csv'\n","  data_frame = pd.read_csv(labels_path)\n","  data_frame = data_frame[data_frame['Worker'] == 'AGGR']\n","  return data_frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPTdbfy9UfWb"},"source":["def set_data(trait = 'Overall'):\n","  \n","  all_interviews = np.load('/content/drive/MyDrive/College/Grad Project/MIT/interview_features.npy')\n","  print(all_interviews.shape)\n","  #all_interviews = all_interviews.reshape(all_interviews.shape[0] , all_interviews.shape[1] * all_interviews.shape[2])\n","  #padded_array = np.zeros((all_interviews.shape[0] , 180))\n","  #padded_array[: , :120] = all_interviews\n","  #all_interviews = padded_array\n","  #print(all_interviews.shape)\n","  labels = get_labels()[trait]\n","  \n","\n","  P_data , PP_data , P_labels , PP_labels = all_interviews[0:69 , : ] , all_interviews[69: , : ] , labels[0:69] , labels[69:]\n","  P_data , PP_data , P_labels , PP_labels = shuffle(P_data , PP_data , P_labels , PP_labels , random_state = 2)\n","\n","  del(all_interviews)\n","  del(labels)\n","\n","  train_data , test_data = np.concatenate((P_data[0:55 , :] , PP_data[0:55 , :])) , np.concatenate((P_data[55: , :] , PP_data[55: , :]))\n","  train_labels, test_labels = np.concatenate((P_labels[0:55] , PP_labels[0:55])), np.concatenate((P_labels[55:] , PP_labels[55:]))\n","\n","  return train_data , test_data , train_labels , test_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXKiGdMlUfWc"},"source":["def training_SVR(trait = 'Overall'):\n","  \n","  train_data , test_data , train_labels , test_labels = set_data(trait)\n","  print(\"train_data: \", train_data.shape)\n","  print(\"test_data: \", test_data.shape)\n","\n","  train_data = train_data.reshape(train_data.shape[0] , train_data.shape[1] * train_data.shape[2])\n","  test_data = test_data.reshape(test_data.shape[0] , test_data.shape[1] * test_data.shape[2])\n","\n","  scaler = StandardScaler()\n","  scaler.fit(train_data)\n","  train_data = scaler.transform(train_data)\n","  test_data = scaler.transform(test_data)\n","\n","  train_data = train_data.reshape(train_data.shape[0] , 1 , 180)\n","  test_data = test_data.reshape(test_data.shape[0] , 1, 180)\n","  \n","  #val_data , val_labels = test_data[:14 ,:] , test_labels[:14]\n","  #test_data , test_labels = test_data[14: ,:] , test_labels[14:]\n","\n","  #param_grid = [{'kernel': ['linear' , 'rbf']}]\n","  #params = {'batch_size': [16 , 32]} \n","  #        'epochs': [50, 100]}\n","\n","  grids = estimator\n","  #grids = GridSearchCV(estimator , params,cv=10)\n","\n","  grids.fit(train_data , train_labels) #, validation_data = (val_data , val_labels))\n","  print(grids.score(test_data , test_labels))\n","  Y_predicted = grids.predict(test_data)\n","  # Y_predicted = np.mean(Y_predicted , axis=1)\n","  correlation_res , _ = pearsonr(Y_predicted , test_labels)\n","  print('--------------------------------' + trait + '--------------------------------')\n","  print(correlation_res)\n","  print(test_labels)\n","  print(Y_predicted)\n","\n","  plt.scatter(Y_predicted , test_labels)\n","\n","  return grids  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPLxmX9pTTTB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnKHObNtTTQz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-P79VoATTOh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRXjWzRTTTLb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHQxM3oFTTIw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sd3pXxQTTGM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjx03L6oTTDP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwWklBx_TS_9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFvX4vTKTS59"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSOGEHtDTSvP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-yYkAiEKcLI","executionInfo":{"status":"ok","timestamp":1620756350364,"user_tz":-120,"elapsed":6147,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"41fa1c32-1e70-472c-8634-308880b1ea9e"},"source":["pip install python_speech_features"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting python_speech_features\n","  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n","Building wheels for collected packages: python-speech-features\n","  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp37-none-any.whl size=5887 sha256=d306b90307043e0ee428e504602ef22a0b48f5c63ba86f0a7cf5db79f0dd137e\n","  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n","Successfully built python-speech-features\n","Installing collected packages: python-speech-features\n","Successfully installed python-speech-features-0.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"outGB6dZ05qr"},"source":["from tensorflow import keras\n","from keras.layers import Dense , Dropout\n","from keras.wrappers.scikit_learn import KerasRegressor\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","from python_speech_features import logfbank , delta\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from scipy.stats import pearsonr\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.utils import shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_bijQMV0WlZ"},"source":["pretrained_model_path = '/content/drive/MyDrive/College/Grad Project/MIT/Pretrained Models/Emotion_Voice_Detection_Model.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNGp8vq40ngY","executionInfo":{"status":"ok","timestamp":1620761490352,"user_tz":-120,"elapsed":751,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"82b7049b-d142-4b84-c278-9f5d3f73e0e7"},"source":["pretrained_model = keras.models.load_model(pretrained_model_path)\n","print(len(pretrained_model.layers))\n","pretrained_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["18\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_7 (Conv1D)            (None, 216, 128)          768       \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 216, 128)          0         \n","_________________________________________________________________\n","conv1d_8 (Conv1D)            (None, 216, 128)          82048     \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 216, 128)          0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 216, 128)          0         \n","_________________________________________________________________\n","max_pooling1d_2 (MaxPooling1 (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_9 (Conv1D)            (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_10 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_10 (Conv1D)           (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_11 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_11 (Conv1D)           (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_12 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 27, 128)           0         \n","_________________________________________________________________\n","conv1d_12 (Conv1D)           (None, 27, 128)           82048     \n","_________________________________________________________________\n","activation_13 (Activation)   (None, 27, 128)           0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 3456)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                34570     \n","_________________________________________________________________\n","activation_14 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 445,578\n","Trainable params: 445,578\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iIjfKMmh1G_w"},"source":["#pretrained_model.layers.pop()\n","#print(len(pretrained_model.layers))\n","#pretrained_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IlTHHGz48vj"},"source":["def my_model():\n","  new_pretrained_model = pretrained_model #keras.Sequential(pretrained_model.layers[:-1])   #pretrained_model\n","  for layer in new_pretrained_model.layers[:-1]:\n","    layer.trainable = False\n","  \n","  # new_pretrained_model.add(Dense(16 , activation='relu' , name='aaaa'))\n","  # new_pretrained_model.add(Dropout(0.25 , name = 'dddd'))\n","  #new_pretrained_model.add(Dense(1 , kernel_regularizer=keras.regularizers.l2(l2=0.01) , name='bbb'))\n","  new_pretrained_model.compile(loss='mean_squared_error' , optimizer = 'adam')#optimizer=keras.optimizers.Adam(learning_rate=0.001))\n","  return new_pretrained_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHzeJum_7eXQ"},"source":["estimator = KerasRegressor(build_fn=my_model , epochs=300 , batch_size = 32 , verbose=1 )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfgQR4SLcKT8"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"rIh5cObmfpbD"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import scipy.io.wavfile as wav\n","import os\n","from os import path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keul_E4C0rto","executionInfo":{"status":"ok","timestamp":1620761495870,"user_tz":-120,"elapsed":662,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"65d7627b-caa2-4941-d94c-d31b3be65cb2"},"source":["cd '/content/drive/MyDrive/College/Grad Project/MIT/Audios_Answers'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1vMfKGnS1lhSz-f1ELRTPdbnHyeg69WAI/Grad Project/MIT/Audios_Answers\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bof0lm7-cTu2"},"source":["def get_labels():\n","  labels_path = '/content/drive/MyDrive/College/Grad Project/MIT/turker_scores_full_interview.csv'\n","  data_frame = pd.read_csv(labels_path)\n","  data_frame = data_frame[data_frame['Worker'] == 'AGGR']\n","  return data_frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRvvw8KBc0lC"},"source":["def set_data(trait = 'Overall'):\n","  \n","  all_interviews = np.load('/content/drive/MyDrive/College/Grad Project/MIT/interview_features.npy')\n","  print(all_interviews.shape)\n","  #all_interviews = all_interviews.reshape(all_interviews.shape[0] , all_interviews.shape[1] * all_interviews.shape[2])\n","  #padded_array = np.zeros((all_interviews.shape[0] , 180))\n","  #padded_array[: , :120] = all_interviews\n","  #all_interviews = padded_array\n","  #print(all_interviews.shape)\n","  labels = get_labels()[trait]\n","  \n","\n","  P_data , PP_data , P_labels , PP_labels = all_interviews[0:69 , : ] , all_interviews[69: , : ] , labels[0:69] , labels[69:]\n","  P_data , PP_data , P_labels , PP_labels = shuffle(P_data , PP_data , P_labels , PP_labels , random_state = 2)\n","\n","  del(all_interviews)\n","  del(labels)\n","\n","  train_data , test_data = np.concatenate((P_data[0:55 , :] , PP_data[0:55 , :])) , np.concatenate((P_data[55: , :] , PP_data[55: , :]))\n","  train_labels, test_labels = np.concatenate((P_labels[0:55] , PP_labels[0:55])), np.concatenate((P_labels[55:] , PP_labels[55:]))\n","\n","  return train_data , test_data , train_labels , test_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gO3tm8-Gc1mL"},"source":["def training_SVR(trait = 'Overall'):\n","  \n","  train_data , test_data , train_labels , test_labels = set_data(trait)\n","  print(\"train_data: \", train_data.shape)\n","  print(\"test_data: \", test_data.shape)\n","\n","  train_data = train_data.reshape(train_data.shape[0] , train_data.shape[1] * train_data.shape[2])\n","  test_data = test_data.reshape(test_data.shape[0] , test_data.shape[1] * test_data.shape[2])\n","\n","  scaler = StandardScaler()\n","  scaler.fit(train_data)\n","  train_data = scaler.transform(train_data)\n","  test_data = scaler.transform(test_data)\n","\n","  train_data = train_data.reshape(train_data.shape[0] , 1 , 180)\n","  test_data = test_data.reshape(test_data.shape[0] , 1, 180)\n","  \n","  #val_data , val_labels = test_data[:14 ,:] , test_labels[:14]\n","  #test_data , test_labels = test_data[14: ,:] , test_labels[14:]\n","\n","  #param_grid = [{'kernel': ['linear' , 'rbf']}]\n","  #params = {'batch_size': [16 , 32]} \n","  #        'epochs': [50, 100]}\n","\n","  grids = estimator\n","  #grids = GridSearchCV(estimator , params,cv=10)\n","\n","  grids.fit(train_data , train_labels) #, validation_data = (val_data , val_labels))\n","  print(grids.score(test_data , test_labels))\n","  Y_predicted = grids.predict(test_data)\n","  # Y_predicted = np.mean(Y_predicted , axis=1)\n","  correlation_res , _ = pearsonr(Y_predicted , test_labels)\n","  print('--------------------------------' + trait + '--------------------------------')\n","  print(correlation_res)\n","  print(test_labels)\n","  print(Y_predicted)\n","\n","  plt.scatter(Y_predicted , test_labels)\n","\n","  return grids  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"Eet4xdnYy-iE","executionInfo":{"status":"error","timestamp":1620761676987,"user_tz":-120,"elapsed":6164,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"34a36412-f5bc-4662-d3e7-74ad643ef20e"},"source":["overall = training_SVR('Overall')\n","# overall.save(\"AHNPS-c-LSTM_overall.h5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(138, 2500, 180)\n","train_data:  (110, 2500, 180)\n","test_data:  (28, 2500, 180)\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-106-bb7e9a1a35e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_SVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# overall.save(\"AHNPS-c-LSTM_overall.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-105-d8c4e1ffd5d8>\u001b[0m in \u001b[0;36mtraining_SVR\u001b[0;34m(trait)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 49500000 into shape (110,1,180)"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"3p-j2lm1eLWH","executionInfo":{"status":"error","timestamp":1620758128761,"user_tz":-120,"elapsed":610,"user":{"displayName":"Arsany Atef","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnWLay54uiaokBHn8Qu6mcClJn-2QDu363_aYQcQ=s64","userId":"18018075603714929228"}},"outputId":"927f97b8-bf1e-4e3b-ff1f-64395c5add52"},"source":["overall.save(\"AHNPS-c-LSTM_overall.h5\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-270766a214de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AHNPS-c-LSTM_overall.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'save'"]}]},{"cell_type":"code","metadata":{"id":"eu9RvoPDeORw"},"source":[""],"execution_count":null,"outputs":[]}]}